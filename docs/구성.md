# 쿠버네티스 하드웨이

쿠버네티스는 내부에 많은 프로그램들로 구성되는데, 이들 프로그램들 간 통신은 보안통신으로 기반한다.    
이를 위해서 TLS 인증서가 필요하기 대문에 여기서는 cfssl, cfssljson 명령어를 사용할 것이다.

<br>

## 환경 구성.
Master, Worker node 각 2개, Proxy 서버 1개 총 5개의 서버를 운영할 것이다.

<br>

<br>

## Install CFSSL 

<br>

```
wget -q https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssl \
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssljson
```

```
chmod +x cfssl cfssljson
```

```
sudo mv cfssl cfssljson /usr/local/bin/
```


<h2> Verification </h2>

<br>

```
cfssl version
```
```
cfssljson --version
```
> output

```
Version: 1.4.1
Runtime: go1.12.12 # 둘 다 같은 출력 값이 나옴.
```

<br>

<h2> Install kubectl </h2>

<br>

kubectl은 kubernetes api 서버와 통신하기 위해 설치한다.

<br>

```
wget https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl
```

```
chmod +x kubectl
```

```
sudo mv kubectl /usr/local/bin
```

<h2> Verification </h2>
<br>

```
kubectl version --client
```
> output
```
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.0", GitCommit:"cb303e613a121a29364f75cc67d3d580833a7479", GitTreeState:"clean", BuildDate:"2021-04-08T16:31:21Z", GoVersion:"go1.16.1", Compiler:"gc", Platform:"linux/amd64"}
```

<br>

<h2> SSH 인증키 생성 </h2>

<br>

```
ssh-keygen -t rsa (enter)
```

***
작업할 가상머신 가서 `# mkdir .ssh` (.ssh 폴더 만들어주기)
***

```
scp .ssh/id_rsa.pub root@192.168.5.100:~/.ssh/authorized_keys
scp .ssh/id_rsa.pub root@192.168.5.101:~/.ssh/authorized_keys
scp .ssh/id_rsa.pub root@192.168.5.110:~/.ssh/authorized_keys
scp .ssh/id_rsa.pub root@192.168.5.111:~/.ssh/authorized_keys
```
***
각 서버에서 작업
***
```
sudo hostnamectl set-hostname master1
sudo hostnamectl set-hostname master2
sudo hostnamectl set-hostname worker1
sudo hostnamectl set-hostname worker2
sudo hostnamectl set-hostname haproxy
```

Host 파일 구성
```
cat << EOF >> /etc/hosts

192.168.5.100    master1
192.168.5.101    master2
192.168.5.110    worker1
192.168.5.111    worker2
192.168.5.150    haproxy

EOF
```
<br>

<h2> Provisioning a CA and Generating TLS Certificates </h2>   

<br>

TLS 증명서를 생성하기 위해서 CA 프로비저닝 하기.

-> HTTPS 통신을 하기 위해서는 인증서가 필요하고 이것을 공공기관에서 발급받을 수가 없으니 개인적으로 만들어 주는 것.
```
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

> ouput
```
ca.csr
ca-key.pem
ca.pem
```
ca.pem은 인증서, ca-key.pem은 Root CA에서 사용할 개인키, ca.car은 인증 요청서다.    
Self Root CA 조차도 Key를 요청하는 것이기 때문에 csr이 필요해서 만들어진 것. 

<br>

## Client and Server Cerificates

<br>

각 Kubernetes 컴포넌트의 클라이언트 증명서와 서버 증명서를 생성하고 Kubernetes 관리자 사용자의 클라이언트 증명서를 생성합니다.

-> apiserver 클러스터 관리자 인증이 필요하기 때문에.

<br>

### The Adimin Client Certificate

<br>

```
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin
```

> output
```
admin-key.pem
admin.pem
```
<br>

### The Kubelet Client Certificates

<br>
kubernetes는 kubelet에 의해 이루어지는 API 요구를 인가해주는 Node Authorizer라고 불리는 특별한 인가 모드를 사용한다. 

노드 인증(Node Authorizer) 로 인증되기 위해, Kubelets 는 반드시 `system:node:<nodename>` 의 이름을 가진 system:nodes 그룹에 그들이 있는 것처럼 확인된 자격증명을 사용해야 한다.

따라서 nodename 이 반드시 있어야 한다. 만일 DNS 서버가 없다고 해도 괜찮다. 쿠버네티스 클러스터를 위한 서버들에 /etc/hosts 에 정적으로 도메인을 할당하면 되며, 호스트도 마찬가지로 정적으로 넣어줘도 된다.

```
for instance in worker1 worker2; do
cat > ${instance}-csr.json <<EOF
{
  "CN": "system:node:${instance}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

EXTERNAL_IP=192.168.1.150  # haproxy IP
INTERNAL_IP=("192.168.1.110" "192.168.1.111") # Worker node IP

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${instance},${EXTERNAL_IP},${INTERNAL_IP} \
  -profile=kubernetes \
  ${instance}-csr.json | cfssljson -bare ${instance}
done
```
>  output
```
worker1-key.pem
worker1.pem
worekr2-key.pem
worekr2.pem
```

<br>

### The Controller Manager Client Certificate

<br>

```
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
```
> output
```
kube-controller-manager-key.pem
kube-controller-manager.pem
```
<br>

### The Kube Proxy Client Certificate

<br>

```
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy
```
> output
```
kube-proxy-key.pem
kube-proxy.pem
```

<br>

### The Scheduler Client Certificate

<br>

```
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler
```
> output
```
kube-scheduler-key.pem
kube-scheduler.pem
```

<br>

### The Kubernetes API Server Certificate

<br>

Kubernetes API Server 인증서라고 설명되어 있지만, API Server 는 쿠버네티스 컨트롤 플레인 노드에 있게 된다. 그리고 로드 밸런서를 통해서 워커 노드와 통신을 하게 된다. 이뿐만 아니라 API Server 는 외부에 클라이언트와 Kubectl 을 통해서 통신도 해야 한다.

이를 위해서 인증서에는 쿠버네티스 컨트롤 플레인 서버들과 로드밸런서에 대해 각각 인증이 가능해야 한다.

<br>

```
EXTERNAL_IP=192.168.5.150
KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}

EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.32.0.1,192.168.5.100,192.168.5.101,127.0.0.1,${KUBERNETES_HOSTNAMES},${EXTERNAL_IP} \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
```
> output

```
kubernetes-key.pem
kubernetes.pem
```

-hostname 옵션을 살펴보면 master ip와 localhost ip, 10.32.0.1을 입력해 주었는데, 10.32.0.1은 나중에 Controlpalne을 만들 대에 사용할 Clustert IP 주소 대역인 10..32.0.0/16에 첫 번재 주소다.

쿠버네티스 API 서버는 내부 DNS 네임에 kubernetes 를 Cluster IP 대역의 첫번째 IP와 묶어서 저장해놓는다. 따라서 이 10.32.0.1 주소는 Cluster IP 대역과 연관이 돼 있다는 것을 알아야 한다.


<br>

## The Service Account Key Pair

<br>

Kubernetes Controller Manager는 키 페어를 사용하여 서비스 계정 토큰을 생성하고 서명하기 때문에 설정.

<br>

```
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
```
> output
```
service-account-key.pem
service-account.pem
```
여기까지 필요한 인증서는 다 작성함.
***
<br>

## 인증서 배포

<br>

### Worker Node에 배포

<br>

```
cat << EOF > deployW.sh
#!/bin/bash
for instance in worker1 worker2; do
  scp ca.pem ${instance}-key.pem ${instance}.pem ${instance}:~/
done
EOF

chmod +x deployW.sh
./deployW.sh
```

***
kube-proxy 인증서와 키도 같이 보내자
***

<br>

### Master Node에 배포

<br>

```
cat << EOF > deployM.sh
#!/bin/bash
for instance in master1 master2; do
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem ${instance}:~/
done
EOF

chmod +x deployM.sh
./deployM.sh
```

<br>

## 클라이언트 인증 설정

<br>

controller manager, kubelet, kube-proxy, scheduler 클라이언트 그리고 admin 사용자를 위한 kubeconfig 파일을 생성해준다.

<br>

### Kubernetes Publice IP Address

<br>

kubeconfig는 쿠버네티스 API Server 접속에 필요하고 고가용성을 위해 API Server 앞에 외부 Loadbalancer에 IP 주소를 할당해서 사용함 -> 쿠버네티스 공인 IP 주소.   

우리는 HAProxt의 IP주소가 공인 IP 주소임

```
KUBERNETES_PUBLIC_ADDRESS=192.168.5.150
```
<br>

### The kubelet Kubernetes Configuration File

<br>

Kublet에 대한 kubeconfig 파일을 생성할 때 kubelet의 노드 이름과 일치하는 Client 인증서를 사용행햐 한다.
안그러면 Kubelet이 kubernetes Node Authorizer에 의해 허락되지 않는다.


SSL 증명서를 생성할 대 사용한 것과 같은 디렉터리에서 실행해야 한다.

```
for instance in worker1 worker2; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done
```
> output
```
worker1.kubeconfig
worker2.kubeconfig
```

<br>

### The kube-protx Kubernetes Configuration File

<br>

```
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```
> output
```
kube-proxy.kubeconfig
```

<br>

### The kube-controller-manager Kubernetes Configuration File

<br>

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
```
> output
```
kube-controller-manager.kubeconfig
```
<br>

### The kube-scheduler Kubernetes Configuration File

<br>

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
```
> output
```
kube-scheduler.kubeconfig
```
<br>

### The admin Kubernetes Configuration File

<br>

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig
```
> output
```
admin.kubeconfig
```
<br>

## Configuration Files 배포하기

<br>

### Worker Node에 배포

<br>

```
cat << EOF > deployConfW.sh
#!/bin/bash
for instance in worker1 worker2; do
  scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/
done
EOF

chmod +x deployConfW.sh
./deployConfW.sh
```

<br>

### Master Node에 배포

<br>

```
cat << EOF > deployConfM.sh
#!/bin/bash
for instance in master1 master2; do
  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
done
EOF

chmod +x deployConfM.sh
./deployConfM.sh
```

<br>

## 데이터 암호화 설정 및 키 생성

<br>

### Key 생성

```
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
```

### 암화화 설정 파일

```
cat > encryption-provider-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

<br>

### Master Node에 배포

<br>

```
scp encryption-provider-config.yaml 192.168.5.100:~/
scp encryption-provider-config.yaml 192.168.5.101:~/
```

<br>

## HAProxy Server 설치 및 설정 
proxy에서 설정.

<br>

```
vi ./haproxy.sh
#!/bin/bash

echo "START"
echo "SET OS ENV"
systemctl disable firewalld

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0

sleep 2
echo ""

echo "INSTALL HAPROXY"
yum install -y haproxy
systemctl enable haproxy

sleep 2
echo ""


INTERNAL_IP=$(ip addr | grep global | awk '{print $2}' | cut -d/ -f1)


### Kubernetes Multi Master Proxy Config ###
echo "HAPROXY Config"
cat << EOF >> /etc/haproxy/haproxy.cfg

frontend k8s
       bind 	${INTERNAL_IP}:6443
       option tcplog
       mode tcp
       default_backend k8s-backend
       
backend k8s-backend
       mode        tcp
       balance     roundrobin
       option      tcp-check
       server      master1 192.168.5.100:6443 check fall 3 rise 2
       server      master2 192.168.5.101:6443 check fall 3 rise 2
EOF
sleep 2
echo ""

echo "HAPROXY Service RESTART"
systemctl restart haproxy
netstat -lpn | grep 6443

echo ""
echo "HAPRORY INSTALL EDN"
echo ""
```
```
chmod +x ./haproxy
./haproxy
```

<br>

## ETCD Server 설치

<br>

### Download and Install the etcd Binaries

<br>

```
wget -q "https://github.com/etcd-io/etcd/releases/download/v3.4.15/etcd-v3.4.15-linux-amd64.tar.gz"

tar -xvf etcd-v3.4.15-linux-amd64.tar.gz
sudo mv etcd-v3.4.15-linux-amd64/etcd* /usr/local/bin/
```

<br>

### ETCD Server 구성

<br>

ETCD 서버는 클러스터 내부의 서버와 master, worker 노드와 통신을 해야한다. 쿠버네티스는 기본적으로 TLS 기반으로 통신하기 때문에, 인증서가 필요하므로 인증서를 복사한 것.

```
sudo mkdir -p /etc/etcd /var/lib/etcd
sudo chmod 700 /var/lib/etcd
sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
```
```
INTERNAL_IP=$(ip addr | grep global | awk '{print $2}' | cut -d/ -f1)
ETCD_NAME=$(hostname -s)

cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master1=https://192.168.5.100:2380,master2=https://192.168.5.101:2380\\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
***
ETCD_Server는 각 MasterNode에서 ETCD_NAME과, INTERNAL_IP만 변경해서 설정해주면 된다.
***
`ip addr | grep global | awk '{print $2}' | cut -d/ -f1`를 쳤을 때 ip가 두 개가 뜬다면, 인터페이스가 하나 더 있는 것이다. (virbr0 있는지 확인)   
-> 제거해주자 참고 : <link>https://fliedcat.tistory.com/167</link> 
***

<br>

### Start the ETCD Server

<br>

```
sudo systemctl daemon-reload
sudo systemctl enable etcd
sudo systemctl start etcd
```

### Verification

```
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem
```
> output
```
695eb17d22613dac, started, master2, https://192.168.5.101:2380, https://192.168.5.101:2379, false
a17abad8824406a7, started, master1, https://192.168.5.100:2380, https://192.168.5.100:2379, false
```

***
1. etcd가 시작이 안되는 오류가 발생함.

2. cat /var/log/messages로 오류확인

3. `openssl x509 -noout -text -in /etc/etcd/kubernetes.pem`   
에서 찾아봤을 때 master2의 ip주소가 없는 것을 확인할 수 있었음.
즉, 인증서를 만들 때 빼먹은 부분이 있었다는 것. 오류가 나면 인증서 부분이 잘못됐을 확률이 높다. 잘 고쳐보자
***

<br>

## Bootstrapping the Kubernetes Control Plane

Control Plane의 주요 구성 요소
1. kube-apiserver
2. kube-controller-manager
3. kube-scheduler
4. kubectl

<br>

### 쿠버네티스 Control Plane 프로비전 - 쿠버네티스 컨트롤 바이너리 다운로드 후 설치

<br>

```
sudo mkdir -p /etc/kubernetes/config
```
```
wget -q \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-scheduler" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl"
```
```
chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
```

<br>

### Kubernetes API 서버 설정

<br>

```
sudo mkdir -p /var/lib/kubernetes/
```

```
sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
  service-account-key.pem service-account.pem \
  encryption-config.yaml /var/lib/kubernetes/
```
```
INTERNAL_IP=$(ip addr | grep global | awk '{print $2}' | cut -d/ -f1)
MASTER1_IP=192.168.5.100
MASTER2_IP=192.168.5.101
```

<br>

### Create the `kube-apiserver.service` 시스템 유닛 파일

```
cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Doc umentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --endpoint-reconciler-type=master-count \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://$MASTER1_IP,https://$MASTER2_IP:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-provider-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --runtime-config=api/all=true \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-account-issuer=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Kubernetes 컨트롤러 매니저 구성하기

<br>

Move the `kube-controller-manager` kubeconfig into place:

```
sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
```

Create the `kube-controller-manager.service` systemd unit file:

```
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
- –cluster-cidr 은 kubeadm 을 이용해 설치할때에 –pod-network-cidr 에 해당한다.

<br>

### Kubernetes Scheduler 구성하기.

<br>

Move the `kube-scheduler` kubeconfig into place:

```
sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/
```

Create the `kube-scheduler.yaml` configuration file:

```
cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF
```

Create the `kube-scheduler.service` systemd unit file:

```
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Controller 서비스 시작

<br>

```
sudo systemctl daemon-reload
sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
```

<br>

## Kubelet 인증을 위한 RBAC

<br>

쿠버네티스는 kubectl 을 이용해 명령을 받는다. 그러면 api server 에서 받고, 이것을 워커 서버에 kubelet 이 실행시키는 구조다. 문제는 api server 가 워커 노드의 kubelet 에 액세스를 할려면 RBAC 권한이 필요하다는 데 있다. RBAC(Role-Based Access Control) 는 사용자 역할 기반 접근 제어 방법이다.

### kube-apiserver-to-kubelet ClusterRole 생성하기.

<br>

```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF
```

<br>

### Bind ClusterRole to the 쿠버네티스 사용자.

<br>

```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```
쿠버네티스 API 서버는 –kubelet-client-certificate 플래그에 정의된 클라이언트 인증서를 사용해 kubernetes 사용자로 Kubelet 에 인증한다.

앞에서 생성한 ClusterRole 을 kubernetes 사용자에게 바인딩(binding) 해준다.

<br>

### Verification

<br>

컨트롤 플레인이 잘 설정 되었는지 다음과 같이 확인할 수 있다.

```
kubectl get componentstatuses --kubeconfig admin.kubeconfig -o yaml | egrep "kind|name|message"
```
> output
```
Warning: v1 ComponentStatus is deprecated in v1.19+
  - message: ok
  kind: ComponentStatus
    name: controller-manager
  - message: ok
  kind: ComponentStatus
    name: scheduler
  - message: '{"health":"true"}'
  kind: ComponentStatus
    name: etcd-0
  - message: '{"health":"true"}'
  kind: ComponentStatus
    name: etcd-1
```

<br>

로드 밸런서에서 응답이 정상으로 나오는지도 확인할 수 있다.

<br>

```
KUBERNETES_PUBLIC_ADDRESS=192.168.5.150
curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version
```
> output
```
"major": "1",
"minor": "21",
"gitVersion": "v1.21.0",
"gitCommit": "cb303e613a121a29364f75cc67d3d580833a7479",
"gitTreeState": "clean",
"buildDate": "2021-04-08T16:25:06Z",
"goVersion": "go1.16.1",
"compiler": "gc",
"platform": "linux/amd64"
```

<br>

## Bootstrapping the Kubernetes Worker Nodes

<br>

각 Worker Node에 설치할 것임.

### Woker-Node 프로비저닝

<br>

Install OS Dependencies:

```
yum -y update
yum -y install socat conntrack ipser
yum -y install 
```

socat은 kubectl port-forward를 지원해준다.
***

<br>

### Disable Swap

쿠버네티스는 Pod를 생성할 때, 필요한 만큼의 리소스를 할당 받아서 사용하는 구조다. 따라서 메모리 Swap을 고려하지 않고 설계되었기 때문에, 쿠버네티스 클러스터 Node들은 모두 Swap 메모리를 비활성화 해줘야 한다.
```
swapoff
```

### Install Docker

Container Engine인 Docker를 설치해줘야 한다.

1. Set up the repository
<pre>
sudo yum install -y yum-utils
sudo yum-config-manager \
  --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo
</pre>

2. Docker Engine 설치
<pre>
sudo yum -y install docker-ce docker-ce-cli containerd.io
</pre>

3. 실행 후 설치되었는지 확인.
<pre>
systemctl start docker && systemctl enable docker
systemctl status docker 
</pre>

### Worker Node 작동을 위한 Binaries Download and Install

```
wget -q --show-progress --https-only --timestamping \
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.21.0/crictl-v1.21.0-linux-amd64.tar.gz \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc93/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz \
  https://github.com/containerd/containerd/releases/download/v1.4.4/containerd-1.4.4-linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubelet
```

설치 파일 생성.

<br>

```
sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```

