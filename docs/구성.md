# 쿠버네티스 하드웨이

쿠버네티스는 내부에 많은 프로그램들로 구성되는데, 이들 프로그램들 간 통신은 보안통신으로 기반한다.    
이를 위해서 TLS 인증서가 필요하기 대문에 여기서는 cfssl, cfssljson 명령어를 사용할 것이다.

<br>

## 환경 구성.
Master, Worker node 각 2개, Proxy 서버 1개 총 5개의 서버를 운영할 것이다.

<br>

<br>

## Install CFSSL 

<br>

```
wget -q https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssl \
  https://storage.googleapis.com/kubernetes-the-hard-way/cfssl/1.4.1/linux/cfssljson
```

```
chmod +x cfssl cfssljson
```

```
sudo mv cfssl cfssljson /usr/local/bin/
```


<h2> Verification </h2>

<br>

```
cfssl version
```
```
cfssljson --version
```
> output

```
Version: 1.4.1
Runtime: go1.12.12 # 둘 다 같은 출력 값이 나옴.
```

<br>

<h2> Install kubectl </h2>

<br>

kubectl은 kubernetes api 서버와 통신하기 위해 설치한다.

<br>

```
wget https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl
```

```
chmod +x kubectl
```

```
sudo mv kubectl /usr/local/bin
```

<h2> Verification </h2>
<br>

```
kubectl version --client
```
> output
```
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.0", GitCommit:"cb303e613a121a29364f75cc67d3d580833a7479", GitTreeState:"clean", BuildDate:"2021-04-08T16:31:21Z", GoVersion:"go1.16.1", Compiler:"gc", Platform:"linux/amd64"}
```

<br>

<h2> SSH 인증키 생성 </h2>

<br>

```
ssh-keygen -t rsa (enter)
```

***
작업할 가상머신 가서 `# mkdir .ssh` (.ssh 폴더 만들어주기)
***

```
scp .ssh/id_rsa.pub root@192.168.5.100:~/.ssh/authorized_keys
scp .ssh/id_rsa.pub root@192.168.5.101:~/.ssh/authorized_keys
scp .ssh/id_rsa.pub root@192.168.5.110:~/.ssh/authorized_keys
scp .ssh/id_rsa.pub root@192.168.5.111:~/.ssh/authorized_keys
```
***
각 서버에서 작업
***
```
sudo hostnamectl set-hostname master1
sudo hostnamectl set-hostname master2
sudo hostnamectl set-hostname worker1
sudo hostnamectl set-hostname worker2
sudo hostnamectl set-hostname haproxy
```

Host 파일 구성
```
cat << EOF >> /etc/hosts

192.168.5.100    master1
192.168.5.101    master2
192.168.5.110    worker1
192.168.5.111    worker2
192.168.5.150    haproxy

EOF
```
<br>

<h2> Provisioning a CA and Generating TLS Certificates </h2>   

<br>

TLS 증명서를 생성하기 위해서 CA 프로비저닝 하기.

-> HTTPS 통신을 하기 위해서는 인증서가 필요하고 이것을 공공기관에서 발급받을 수가 없으니 개인적으로 만들어 주는 것.
```
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF

cat > ca-csr.json <<EOF
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

> ouput
```
ca.csr
ca-key.pem
ca.pem
```
ca.pem은 인증서, ca-key.pem은 Root CA에서 사용할 개인키, ca.car은 인증 요청서다.    
Self Root CA 조차도 Key를 요청하는 것이기 때문에 csr이 필요해서 만들어진 것. 

<br>

## Client and Server Cerificates

<br>

각 Kubernetes 컴포넌트의 클라이언트 증명서와 서버 증명서를 생성하고 Kubernetes 관리자 사용자의 클라이언트 증명서를 생성합니다.

-> apiserver 클러스터 관리자 인증이 필요하기 때문에.

<br>

### The Adimin Client Certificate

<br>

```
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin
```

> output
```
admin-key.pem
admin.pem
```
<br>

### The Kubelet Client Certificates

<br>
kubernetes는 kubelet에 의해 이루어지는 API 요구를 인가해주는 Node Authorizer라고 불리는 특별한 인가 모드를 사용한다. 

노드 인증(Node Authorizer) 로 인증되기 위해, Kubelets 는 반드시 `system:node:<nodename>` 의 이름을 가진 system:nodes 그룹에 그들이 있는 것처럼 확인된 자격증명을 사용해야 한다.

따라서 nodename 이 반드시 있어야 한다. 만일 DNS 서버가 없다고 해도 괜찮다. 쿠버네티스 클러스터를 위한 서버들에 /etc/hosts 에 정적으로 도메인을 할당하면 되며, 호스트도 마찬가지로 정적으로 넣어줘도 된다.

```
for instance in worker1 worker2; do
cat > ${instance}-csr.json <<EOF
{
  "CN": "system:node:${instance}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

EXTERNAL_IP=192.168.1.150  # haproxy IP
INTERNAL_IP=("192.168.1.110" "192.168.1.111") # Worker node IP

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${instance},${EXTERNAL_IP},${INTERNAL_IP} \
  -profile=kubernetes \
  ${instance}-csr.json | cfssljson -bare ${instance}
done
```
>  output
```
worker1-key.pem
worker1.pem
worekr2-key.pem
worekr2.pem
```

<br>

### The Controller Manager Client Certificate

<br>

```
cat > kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
```
> output
```
kube-controller-manager-key.pem
kube-controller-manager.pem
```
<br>

### The Kube Proxy Client Certificate

<br>

```
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy
```
> output
```
kube-proxy-key.pem
kube-proxy.pem
```

<br>

### The Scheduler Client Certificate

<br>

```
cat > kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler
```
> output
```
kube-scheduler-key.pem
kube-scheduler.pem
```

<br>

### The Kubernetes API Server Certificate

<br>

Kubernetes API Server 인증서라고 설명되어 있지만, API Server 는 쿠버네티스 컨트롤 플레인 노드에 있게 된다. 그리고 로드 밸런서를 통해서 워커 노드와 통신을 하게 된다. 이뿐만 아니라 API Server 는 외부에 클라이언트와 Kubectl 을 통해서 통신도 해야 한다.

이를 위해서 인증서에는 쿠버네티스 컨트롤 플레인 서버들과 로드밸런서에 대해 각각 인증이 가능해야 한다.

<br>

```
EXTERNAL_IP=192.168.5.150
KUBERNETES_HOSTNAMES=kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local

cat > kubernetes-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}

EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.32.0.1,192.168.5.100,192.168.5.101,127.0.0.1,${KUBERNETES_HOSTNAMES},${EXTERNAL_IP} \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
```
> output

```
kubernetes-key.pem
kubernetes.pem
```

-hostname 옵션을 살펴보면 master ip와 localhost ip, 10.32.0.1을 입력해 주었는데, 10.32.0.1은 나중에 Controlpalne을 만들 대에 사용할 Clustert IP 주소 대역인 10..32.0.0/16에 첫 번재 주소다.

쿠버네티스 API 서버는 내부 DNS 네임에 kubernetes 를 Cluster IP 대역의 첫번째 IP와 묶어서 저장해놓는다. 따라서 이 10.32.0.1 주소는 Cluster IP 대역과 연관이 돼 있다는 것을 알아야 한다.


<br>

## The Service Account Key Pair

<br>

Kubernetes Controller Manager는 키 페어를 사용하여 서비스 계정 토큰을 생성하고 서명하기 때문에 설정.

<br>

```
cat > service-account-csr.json <<EOF
{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
```
> output
```
service-account-key.pem
service-account.pem
```
여기까지 필요한 인증서는 다 작성함.
***
<br>

## 인증서 배포

<br>

### Worker Node에 배포

<br>

```
cat << EOF > deployW.sh
#!/bin/bash
for instance in worker1 worker2; do
  scp ca.pem ${instance}-key.pem ${instance}.pem ${instance}:~/
done
EOF

chmod +x deployW.sh
./deployW.sh
```

***
kube-proxy 인증서와 키도 같이 보내자
***

<br>

### Master Node에 배포

<br>

```
cat << EOF > deployM.sh
#!/bin/bash
for instance in master1 master2; do
  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem ${instance}:~/
done
EOF

chmod +x deployM.sh
./  M.sh
```

<br>

## 클라이언트 인증 설정

<br>

controller manager, kubelet, kube-proxy, scheduler 클라이언트 그리고 admin 사용자를 위한 kubeconfig 파일을 생성해준다.

<br>

### Kubernetes Publice IP Address

<br>

kubeconfig는 쿠버네티스 API Server 접속에 필요하고 고가용성을 위해 API Server 앞에 외부 Loadbalancer에 IP 주소를 할당해서 사용함 -> 쿠버네티스 공인 IP 주소.   

우리는 HAProxt의 IP주소가 공인 IP 주소임

```
KUBERNETES_PUBLIC_ADDRESS=192.168.5.150
```
<br>

### The kubelet Kubernetes Configuration File

<br>

Kublet에 대한 kubeconfig 파일을 생성할 때 kubelet의 노드 이름과 일치하는 Client 인증서를 사용행햐 한다.
안그러면 Kubelet이 kubernetes Node Authorizer에 의해 허락되지 않는다.


SSL 증명서를 생성할 대 사용한 것과 같은 디렉터리에서 실행해야 한다.

```
for instance in worker1 worker2; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done
```
> output
```
worker1.kubeconfig
worker2.kubeconfig
```

<br>

### The kube-protx Kubernetes Configuration File

<br>

```
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```
> output
```
kube-proxy.kubeconfig
```

<br>

### The kube-controller-manager Kubernetes Configuration File

<br>

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
```
> output
```
kube-controller-manager.kubeconfig
```
<br>

### The kube-scheduler Kubernetes Configuration File

<br>

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
```
> output
```
kube-scheduler.kubeconfig
```
<br>

### The admin Kubernetes Configuration File

<br>

```
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default --kubeconfig=admin.kubeconfig
```
> output
```
admin.kubeconfig
```
<br>

## Configuration Files 배포하기

<br>

### Worker Node에 배포

<br>

```
cat << EOF > deployConfW.sh
#!/bin/bash
for instance in worker1 worker2; do
  scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/
done
EOF

chmod +x deployConfW.sh
./deployConfW.sh
```

<br>

### Master Node에 배포

<br>

```
cat << EOF > deployConfM.sh
#!/bin/bash
for instance in master1 master2; do
  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/
done
EOF

chmod +x deployConfM.sh
./deployConfM.sh
```

<br>

## 데이터 암호화 설정 및 키 생성

<br>

### Key 생성

```
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
```

### 암화화 설정 파일

```
cat > encryption-provider-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
```

<br>

### Master Node에 배포

<br>

```
scp encryption-provider-config.yaml 192.168.5.100:~/
scp encryption-provider-config.yaml 192.168.5.101:~/
```

<br>

## HAProxy Server 설치 및 설정 
proxy에서 설정.

<br>

```
vi ./haproxy.sh
#!/bin/bash

echo "START"
echo "SET OS ENV"
systemctl disable firewalld

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
setenforce 0

sleep 2
echo ""

echo "INSTALL HAPROXY"
yum install -y haproxy
systemctl enable haproxy

sleep 2
echo ""


INTERNAL_IP=$(ip addr | grep global | awk '{print $2}' | cut -d/ -f1)


### Kubernetes Multi Master Proxy Config ###
echo "HAPROXY Config"
cat << EOF >> /etc/haproxy/haproxy.cfg

frontend k8s
       bind 	${INTERNAL_IP}:6443
       option tcplog
       mode tcp
       default_backend k8s-backend
       
backend k8s-backend
       mode        tcp
       balance     roundrobin
       option      tcp-check
       server      master1 192.168.5.100:6443 check fall 3 rise 2
       server      master2 192.168.5.101:6443 check fall 3 rise 2
EOF
sleep 2
echo ""

echo "HAPROXY Service RESTART"
systemctl restart haproxy
netstat -lpn | grep 6443

echo ""
echo "HAPRORY INSTALL EDN"
echo ""
```
```
chmod +x ./haproxy
./haproxy
```

<br>

## ETCD Server 설치

<br>

### Download and Install the etcd Binaries

<br>

```
wget -q "https://github.com/etcd-io/etcd/releases/download/v3.4.15/etcd-v3.4.15-linux-amd64.tar.gz"

tar -xvf etcd-v3.4.15-linux-amd64.tar.gz
sudo mv etcd-v3.4.15-linux-amd64/etcd* /usr/local/bin/
```

<br>

### ETCD Server 구성

<br>

ETCD 서버는 클러스터 내부의 서버와 master, worker 노드와 통신을 해야한다. 쿠버네티스는 기본적으로 TLS 기반으로 통신하기 때문에, 인증서가 필요하므로 인증서를 복사한 것.

```
sudo mkdir -p /etc/etcd /var/lib/etcd
sudo chmod 700 /var/lib/etcd
sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
```
```
INTERNAL_IP=$(ip addr | grep global | awk '{print $2}' | cut -d/ -f1)
ETCD_NAME=$(hostname -s)

cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master1=https://192.168.5.100:2380,master2=https://192.168.5.101:2380\\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
***
ETCD_Server는 각 MasterNode에서 ETCD_NAME과, INTERNAL_IP만 변경해서 설정해주면 된다.
***
`ip addr | grep global | awk '{print $2}' | cut -d/ -f1`를 쳤을 때 ip가 두 개가 뜬다면, 인터페이스가 하나 더 있는 것이다. (virbr0 있는지 확인)   
-> 제거해주자 참고 : <link>https://fliedcat.tistory.com/167</link> 
***

<br>

### Start the ETCD Server

<br>

```
sudo systemctl daemon-reload
sudo systemctl enable etcd
sudo systemctl start etcd
```

### Verification

```
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem
```
> output
```
695eb17d22613dac, started, master2, https://192.168.5.101:2380, https://192.168.5.101:2379, false
a17abad8824406a7, started, master1, https://192.168.5.100:2380, https://192.168.5.100:2379, false
```

***
1. etcd가 시작이 안되는 오류가 발생함.

2. cat /var/log/messages로 오류확인

3. `openssl x509 -noout -text -in /etc/etcd/kubernetes.pem`   
에서 찾아봤을 때 master2의 ip주소가 없는 것을 확인할 수 있었음.
즉, 인증서를 만들 때 빼먹은 부분이 있었다는 것. 오류가 나면 인증서 부분이 잘못됐을 확률이 높다. 잘 고쳐보자
***

<br>

## Bootstrapping the Kubernetes Control Plane
***
모든 Master_Node에  설정
***
Control Plane의 주요 구성 요소
1. kube-apiserver
2. kube-controller-manager
3. kube-scheduler
4. kubectl

<br>

### 쿠버네티스 Control Plane 프로비전 - 쿠버네티스 컨트롤 바이너리 다운로드 후 설치

<br>

```
sudo mkdir -p /etc/kubernetes/config
```
```
wget -q \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-apiserver" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-controller-manager" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-scheduler" \
  "https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl"
```
```
chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/
```

<br>

### Kubernetes API 서버 설정

<br>

```
sudo mkdir -p /var/lib/kubernetes/
```

```
sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
  service-account-key.pem service-account.pem \
  encryption-config.yaml /var/lib/kubernetes/
```
```
INTERNAL_IP=$(ip addr | grep global | awk '{print $2}' | cut -d/ -f1)
MASTER1_IP=192.168.5.100
MASTER2_IP=192.168.5.101
```

<br>

### Create the `kube-apiserver.service` 시스템 유닛 파일

```
cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Doc umentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://$MASTER1_IP,https://$MASTER2_IP:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-provider-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --runtime-config='api/all=true' \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-account-issuer=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Kubernetes 컨트롤러 매니저 구성하기

<br>

Move the `kube-controller-manager` kubeconfig into place:

```
sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/
```

Create the `kube-controller-manager.service` systemd unit file:

```
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
- –cluster-cidr 은 kubeadm 을 이용해 설치할때에 –pod-network-cidr 에 해당한다.

<br>

### Kubernetes Scheduler 구성하기.

<br>

Move the `kube-scheduler` kubeconfig into place:

```
sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/
```

Create the `kube-scheduler.yaml` configuration file:

```
cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF
```

Create the `kube-scheduler.service` systemd unit file:

```
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Controller 서비스 시작

<br>

```
sudo systemctl daemon-reload
sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
```

***
이렇게 하면 무조건 오류가 뜨게 된다.

1. `systemctl status start kube-apiserver kube-controller-manager kube-scheduler`를 치고 나서 active가 running이 아닌 것을 확인한다.
2. `cat /var/log/messages`를 확인한다.   
api-server가 오류인 것을 확인할 수 있었다.
3. kubernetes 공식 홈페이지에서 api-server에 대한 문서를 찾아봄.
4. `--runtime-config='api/all=true'`이 문제인 것을 알 수 있었다.   
-> 진짜 찾는데 엄청 오래걸림...(도움받아서 해결할 수 있었음)
'' 를 지워주면 된다.   
`--runtime-config=api/all=true`
***

<br>

## Kubelet 인증을 위한 RBAC

<br>

쿠버네티스는 kubectl 을 이용해 명령을 받는다. 그러면 api server 에서 받고, 이것을 워커 서버에 kubelet 이 실행시키는 구조다. 문제는 api server 가 워커 노드의 kubelet 에 액세스를 할려면 RBAC 권한이 필요하다는 데 있다. RBAC(Role-Based Access Control) 는 사용자 역할 기반 접근 제어 방법이다.

### kube-apiserver-to-kubelet ClusterRole 생성하기.

<br>

```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF
```

<br>

### Bind ClusterRole to the 쿠버네티스 사용자.

<br>

```
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```
쿠버네티스 API 서버는 –kubelet-client-certificate 플래그에 정의된 클라이언트 인증서를 사용해 kubernetes 사용자로 Kubelet 에 인증한다.

앞에서 생성한 ClusterRole 을 kubernetes 사용자에게 바인딩(binding) 해준다.

<br>

### Verification

<br>

컨트롤 플레인이 잘 설정 되었는지 다음과 같이 확인할 수 있다.

```
kubectl get componentstatuses --kubeconfig admin.kubeconfig -o yaml | egrep "kind|name|message"
```
> output
```
Warning: v1 ComponentStatus is deprecated in v1.19+
  - message: ok
  kind: ComponentStatus
    name: controller-manager
  - message: ok
  kind: ComponentStatus
    name: scheduler
  - message: '{"health":"true"}'
  kind: ComponentStatus
    name: etcd-0
  - message: '{"health":"true"}'
  kind: ComponentStatus
    name: etcd-1
```

<br>

로드 밸런서에서 응답이 정상으로 나오는지도 확인할 수 있다.

<br>

```
KUBERNETES_PUBLIC_ADDRESS=192.168.5.150
curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version
```
> output
```
"major": "1",
"minor": "21",
"gitVersion": "v1.21.0",
"gitCommit": "cb303e613a121a29364f75cc67d3d580833a7479",
"gitTreeState": "clean",
"buildDate": "2021-04-08T16:25:06Z",
"goVersion": "go1.16.1",
"compiler": "gc",
"platform": "linux/amd64"
```

<br>

## Bootstrapping the Kubernetes Worker Nodes


***
모든 Worker Node에 설치할 것임.
***
<br>

### Woker-Node 프로비저닝

<br>

Worker 노드 작업을 진행할 때 컨테이너 엔진을 선택해줘야 하는데 여기서는 RunC로 사용할 것이다.

<br>

Install OS Dependencies:

```
yum -y update
yum -y install socat conntrack ipser
yum -y install bind-utils net-tools
```
***
socat은 kubectl port-forward를 지원해준다.
***

<br>

### Disable Swap

쿠버네티스는 Pod를 생성할 때, 필요한 만큼의 리소스를 할당 받아서 사용하는 구조다. 따라서 메모리 Swap을 고려하지 않고 설계되었기 때문에, 쿠버네티스 클러스터 Node들은 모두 Swap 메모리를 비활성화 해줘야 한다.
```
swapoff
```
***
<br>

<b> 재부팅시에도 Swap 메모리 끄기 </b>   

<br>

```
vi /etc/fstab

# /dev/mapper/centos-swap swap swap defaults 0 0
# 위 문장 주석처리 해주기
``` 
***

<br>

***
<b> Install Docker </b>

Docker 설치는 <링크 넣어야 함> 확인할 수 있다.
Dcoker로 하고 싶으면 참조하기. (나중에 작성할 것임.)
***

<br>

### Worker Node 작동을 위한 Binaries Download and Install

```
wget -q --show-progress --https-only --timestamping \
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.21.0/crictl-v1.21.0-linux-amd64.tar.gz \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc93/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.9.1/cni-plugins-linux-amd64-v0.9.1.tgz \
  https://github.com/containerd/containerd/releases/download/v1.4.4/containerd-1.4.4-linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubelet
```

설치 파일 생성.

<br>

```
sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```

<br>

worker binaries 설치

<br>

```
mkdir containerd
tar -xvf crictl-v1.21.0-linux-amd64.tar.gz
tar -xvf containerd-1.4.4-linux-amd64.tar.gz -C containerd
sudo tar -xvf cni-plugins-linux-amd64-v0.9.1.tgz -C /opt/cni/bin/
sudo mv runc.amd64 runc
chmod +x crictl kubectl kube-proxy kubelet runc 
sudo mv crictl kubectl kube-proxy kubelet runc /usr/local/bin/
sudo mv containerd/bin/* /bin/
```

<br>

### Configure CNI Networking

<br>

bridge 네트워크 구성파일 생성

```
POD_CIDR=10.200.0.0/16

cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.4.0",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "${POD_CIDR}"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF
```

loopback 네트워크 구성 파일 생성

```
cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.4.0",
    "name": "lo",
    "type": "loopback"
}
EOF
```

<br>

### Configure containerd

<br>

containerd 구성파일 생성

```
sudo mkdir -p /etc/containerd/

cat << EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = "overlayfs"
    [plugins.cri.containerd.default_runtime]
      runtime_type = "io.containerd.runtime.v1.linux"
      runtime_engine = "/usr/local/bin/runc"
      runtime_root = ""
EOF
```

containerd.service 시스템 유닛 파일 생성

```
cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Configure the Kubelet

<br>

```
sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
sudo mv ca.pem /var/lib/kubernetes/
```
***
HOSTNAME에 지정한 hostname이 잘 들어갔는지 확인해주기.   
안들어갔다면, EX) `HOSTNAME=worker1` 식으로 지정.
***

kubelet-config.yaml 구성 파일 생성하기

```
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
```

`resolveConf` 옵션은 `resolve-system`를 실행하고 있는 시스템에서 서비스 디스커버리에 CoreDNS를 사용할 때 루프를 피하기 위해 사용됩니다.

`kubelet.service` 시스템 유닛 파일 생성하기

```
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Configure the kube-proxy

<br>

```
mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
```

kube-proxy-config.yaml 구성 파일 생성하기
```
cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
EOF
```

`kube-proxy.service` 시스템 유닛 파일 생성하기

```
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<br>

### Start the Worker Services

<br>

```
systemctl daemon-reload
systemctl enable containerd kubelet kube-proxy
systemctl start containerd kubelet kube-proxy
```

<br>

### Verification

<br>

잘 작성되었는지 확인하고 싶지만, worker-node에서는
`systemctl stauts containerd kubelet kube-proxy`에서 `Active: active (running)` 표시만 확인할 수 있다.    
그러므로 Master-node에 가서 `kubectl get nodes --kubeconfig admin.kubeconfig` 명령 수행하기
***
※ 참고 ※　　`active`가 되어 있지 않으면 잘못 설정한 것임!!
***
<br>

> output
```
NAME      STATUS     ROLES    AGE     VERSION
worker1   Ready      <none>   1s      v1.21.0
worker2   Ready      <none>   1s      v1.21.0
```
CNI 설정을 해줬기 때문에 STATUS가 Ready인 상태이디.
***

<br>

## 원격 접근을 위한 kubectl 구성

<br>

각 kubeconfig는 접근하기 위한 쿠버네티스 api-server가 필요하다.   
고가용성을 지원하기 위해 api-server 앞에 외부 로드밸런서에 할당된 ip 주소가 사용된다.

<br>

### 관리자 사용자 인증에 적합한 kubeconfig 파일 생성

<br>

이 설정을 해주지 않으면 Master 노드에서 `kubectl` 단독으로 명령어를 사용할 수 없고 뒤에 `--kubeconfig admin.kubeconfig` 옵션을 붙여줘야 한다.

```
KUBERNETES_PUBLIC_ADDRESS=192.168.10.150

kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443

kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem

kubectl config set-context kubernetes-the-hard-way \
  --cluster=kubernetes-the-hard-way \
  --user=admin

kubectl config use-context kubernetes-the-hard-way
```

<br>

### Verification

<br>

```
kubectl version
```
> output
```
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.0", GitCommit:"cb303e613a121a29364f75cc67d3d580833a7479", GitTreeState:"clean", BuildDate:"2021-04-08T16:31:21Z", GoVersion:"go1.16.1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.0", GitCommit:"cb303e613a121a29364f75cc67d3d580833a7479", GitTreeState:"clean", BuildDate:"2021-04-08T16:25:06Z", GoVersion:"go1.16.1", Compiler:"gc", Platform:"linux/amd64"}
```

```
kubectl get nodes
```
> output
```
NAME       STATUS   ROLES    AGE     VERSION
worker1   Ready    <none>   2m35s   v1.21.0
worker2   Ready    <none>   2m35s   v1.21.0
```

<br>

## Provisioning Pod Network Routes

<br>

노드로 스케줄링된 포드는 노드의 포드 CIDR 범위로부터 IP 주소를 받는데, 이 시점에서는 네트워크 루트가 없기 때문에 다른 노드에서 실행되고 있는 다른 포드와 통신할 수 없다.

그렇기 때문에 각 worker-node의 라우팅 테이블에 경로를 설정해줘야 한다.

<br>

### The Routing Table

<br>

> worker1
```
route add -net 10.200.2.0 subnetmask 255.255.255.0 gw 10.200.2.1 
```

> worker2
```
route add -net 10.200.1.0 subnetmask 255.255.255.0 gw 10.200.1.1
```
<br>

## Deploying the DNS Cluster Add-on

<br>

CoreDNS에 의해 지원되는 DNS 기반 서비스 디스커버리를 제공하는 DNS 애드온을 Kubernetes 클러스터 내에서 실행되는 애플리케이션에 도입한다.

### Deploy the `coredns` cluster add-on
```
kubectl apply -f https://storage.googleapis.com/kubernetes-the-hard-way/coredns-1.8.yaml
```
> output
```
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
```

생긴거 확인

```
kubectl get pods -l k8s-app=kube-dns -n kube-system
```
coredns는 kube system이라는 네임스페이스에 생성된다.

> output
```
NAME                       READY   STATUS    RESTARTS   AGE
coredns-8494f9c688-hh7r2   1/1     Running   0          10s
coredns-8494f9c688-zqrj2   1/1     Running   0          10s
```

***
여기서 coredns가 생성은 되지만 Ready가 되지 않는 오류가 발생했음.   
/var/log/messages에서 오류를 찾아보니 resolve에 대한 오류가 발생한 것을 확인했고, 파일이 없어서 발생하는 것.

<br>

-> 해결법
```
yum install -y systemd-resolved
systemctl start systemd-resolved
systemctl enable systemd-resolved
```
***

<br>

### Verification

<br>

busybox POD 생성
```
kubectl run busybox --image=busybox:1.28 --command -- sleep 3600
```

잘 생성되었는지 확인
```
kubectl get pods -l run=busybox
```

> output
```
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          3s
```

busybox 포드의 전체 이름을 검색.
```
POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
```

busybox pod 내의 kubernetes 서비스에 대한 DNS 조회를 실행.
```
kubectl exec -ti $POD_NAME -- nslookup kubernetes
```

> output
```
Server:    10.32.0.10
Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local
```

***
저는 아무리해도 nslookup 부분은 실행이 되지 않고 오류가 나서 일단 넘어갔습니다...
***

<br>

## Smoke Test

<br>

Kubernetes 클러스터가 올바르게 기능하고 있는지 확인하기 위한 일련의 작업을 수행한다.

<br>

### Data Encryption

<br>

In this section you will verify the ability to encrypt secret data at rest.

Generic secret 생성하기.
```
kubectl create secret generic kubernetes-the-hard-way \
  --from-literal="mykey=mydata"
```

etcd에 저장된 kubernetes-the-hardway secret의 16진수 덤프를 출력.
```
ETCDCTL_API=3 etcdctl get \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem\
  /registry/secrets/default/kubernetes-the-hard-way | hexdump -C"
```
> output
```
00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6b 75 62 65 72 6e  |s/default/kubern|
00000020  65 74 65 73 2d 74 68 65  2d 68 61 72 64 2d 77 61  |etes-the-hard-wa|
00000030  79 0a 6b 38 73 3a 65 6e  63 3a 61 65 73 63 62 63  |y.k8s:enc:aescbc|
00000040  3a 76 31 3a 6b 65 79 31  3a 97 d1 2c cd 89 0d 08  |:v1:key1:..,....|
00000050  29 3c 7d 19 41 cb ea d7  3d 50 45 88 82 a3 1f 11  |)<}.A...=PE.....|
00000060  26 cb 43 2e c8 cf 73 7d  34 7e b1 7f 9f 71 d2 51  |&.C...s}4~...q.Q|
00000070  45 05 16 e9 07 d4 62 af  f8 2e 6d 4a cf c8 e8 75  |E.....b...mJ...u|
00000080  6b 75 1e b7 64 db 7d 7f  fd f3 96 62 e2 a7 ce 22  |ku..d.}....b..."|
00000090  2b 2a 82 01 c3 f5 83 ae  12 8b d5 1d 2e e6 a9 90  |+*..............|
000000a0  bd f0 23 6c 0c 55 e2 52  18 78 fe bf 6d 76 ea 98  |..#l.U.R.x..mv..|
000000b0  fc 2c 17 36 e3 40 87 15  25 13 be d6 04 88 68 5b  |.,.6.@..%.....h[|
000000c0  a4 16 81 f6 8e 3b 10 46  cb 2c ba 21 35 0c 5b 49  |.....;.F.,.!5.[I|
000000d0  e5 27 20 4c b3 8e 6b d0  91 c2 28 f1 cc fa 6a 1b  |.' L..k...(...j.|
000000e0  31 19 74 e7 a5 66 6a 99  1c 84 c7 e0 b0 fc 32 86  |1.t..fj.......2.|
000000f0  f3 29 5a a4 1c d5 a4 e3  63 26 90 95 1e 27 d0 14  |.)Z.....c&...'..|
00000100  94 f0 ac 1a cd 0d b9 4b  ae 32 02 a0 f8 b7 3f 0b  |.......K.2....?.|
00000110  6f ad 1f 4d 15 8a d6 68  95 63 cf 7d 04 9a 52 71  |o..M...h.c.}..Rq|
00000120  75 ff 87 6b c5 42 e1 72  27 b5 e9 1a fe e8 c0 3f  |u..k.B.r'......?|
00000130  d9 04 5e eb 5d 43 0d 90  ce fa 04 a8 4a b0 aa 01  |..^.]C......J...|
00000140  cf 6d 5b 80 70 5b 99 3c  d6 5c c0 dc d1 f5 52 4a  |.m[.p[.<.\....RJ|
00000150  2c 2d 28 5a 63 57 8e 4f  df 0a                    |,-(ZcW.O..|
0000015a
```

etcd 키 앞에 k8s:enc:aescbc:v1:key1을 붙여야 합니다.이것은 key1 암호화 키로 데이터를 암호화하기 위해 aescb 공급자가 사용되었음을 나타냅니다.

<br>

### Deployments

<br>

nginx web server 생성하기.

```
kubectl create deployment nginx --image=nginx
```

생성된거 확인
```
kubectl get pods -l app=nginx
```

> output
```
NAME                    READY   STATUS    RESTARTS   AGE
nginx-f89759699-kpn5m   1/1     Running   0          10s
```

<br>

### Port Forwarding

<br>

nginx 포드의 전체 이름을 검색.
```
POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")
```

Local 장치의 포트 8080을 nginx 포드의 포트 80으로 전송.
```
kubectl port-forward #POD_NAME 8080:80
```
> output
```
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
```

***
새로운 터미널 열어야 함.

```
curl --head http://127.0.01:8080
```
> output
```
HTTP/1.1 200 OK
Server: nginx/1.19.10
Date: Sun, 02 May 2021 05:29:25 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 13 Apr 2021 15:13:59 GMT
Connection: keep-alive
ETag: "6075b537-264"
Accept-Ranges: bytes
```
***
이전 단말기로 전환하여 nginx 포드로의 포트 전송을 중지
```
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
```

<br>

### Logs

<br>

nginx pod 로그 출력
```
kubectl logs $POD_NAME
```
> output
```
...
127.0.0.1 - - [02/May/2021:05:29:25 +0000] "HEAD / HTTP/1.1" 200 0 "-" "curl/7.64.0" "-"
```

<br>

### Exec

<br>

nginx 컨테이너에서 nginx -v 명령을 실행하여 nginx 버전을 출력
```
kubectl exec -ti $POD_NAME -- nginx -v
```
> output
```
nginx version: nginx/1.19.10
```

<br>

### Services

<br>

Expose the nginx deployment using a NodePort service
```
kubectl expose deployment nginx --port 80 --type NodePort
```
Retrieve the node port assigned to the nginx service:
```
NODE_PORT=$(kubectl get svc nginx \
  --output=jsonpath='{range .spec.ports[0]}{.nodePort}')
```
Retrieve the external IP address of a worker instance:
Make an HTTP request using the external IP address and the nginx node port:
```
EXTERNAL_IP=192.168.10.150
curl -I http://${EXTERNAL_IP}:${NODE_PORT}
```
> output
```
HTTP/1.1 200 OK
Server: nginx/1.19.10
Date: Sun, 02 May 2021 05:31:52 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 13 Apr 2021 15:13:59 GMT
Connection: keep-alive
ETag: "6075b537-264"
Accept-Ranges: bytes
```



***
실습을 하다보면 workernode로 접근을 못하는 경우가 생기는데, iptable을 flush해줘야 한다.
```
systemctl stop kubelet
iptables --flush
iptables -tnat --flush
systemctl start kubelet
```
***
